{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 9\n",
    "# 0. Reset output\n",
    "!rm -rf /kaggle/working/*\n",
    "\n",
    "# 1. Import v√† c·∫•u h√¨nh¬∂\n",
    "# ===============================\n",
    "# üì¶ IMPORT TH∆Ø VI·ªÜN C∆† B·∫¢N\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# üìä VISUALIZATION\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# ü§ñ MACHINE LEARNING - SKLEARN\n",
    "# ===============================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ‚öñÔ∏è IMBALANCED LEARNING\n",
    "# ===============================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# 2. H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Core Engine)\n",
    "from scipy.stats import skew\n",
    "\n",
    "def extract_features_from_split(csv_path):\n",
    "    if not os.path.exists(csv_path): return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1. T√çNH SNR\n",
    "    df['snr'] = df['Flux'] / (df['Flux_err'] + 1e-6)\n",
    "    \n",
    "    # 2. Th·ªëng k√™\n",
    "    aggs = df.groupby(['object_id', 'Filter']).agg({\n",
    "        'Flux': ['max', 'min', 'mean', 'std', skew],\n",
    "        'snr': ['max', 'mean']\n",
    "    }).unstack()\n",
    "    aggs.columns = [f'{col[0]}_{col[1]}_{col[2]}' for col in aggs.columns]\n",
    "    \n",
    "    # 3. T√≠nh M√†u (Quan tr·ªçng)\n",
    "    if 'Flux_max_g' in aggs.columns and 'Flux_max_r' in aggs.columns:\n",
    "        aggs['color_g_r'] = aggs['Flux_max_g'] - aggs['Flux_max_r']\n",
    "    \n",
    "    # 4. Bi√™n ƒë·ªô\n",
    "    filters = df['Filter'].unique()\n",
    "    for f in filters:\n",
    "        if f'Flux_max_{f}' in aggs.columns and f'Flux_min_{f}' in aggs.columns:\n",
    "            aggs[f'amp_{f}'] = aggs[f'Flux_max_{f}'] - aggs[f'Flux_min_{f}']\n",
    "            \n",
    "    # 5. S·ªë l∆∞·ª£ng quan s√°t\n",
    "    counts = df.groupby('object_id').size().to_frame('n_obs')\n",
    "    \n",
    "    features = aggs.merge(counts, left_index=True, right_index=True)\n",
    "    return features\n",
    "\n",
    "# !!! B·∫†N NH·ªö CH·∫†Y L·∫†I H√ÄM LOAD D·ªÆ LI·ªÜU ƒê·ªÇ C·∫¨P NH·∫¨T full_train NH√â !!!\n",
    "\n",
    "# H√†m h·ªó tr·ª£ load to√†n b·ªô 20 splits\n",
    "def load_all_splits(base_path, mode='train'):\n",
    "    all_features = []\n",
    "    print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu {mode} t·ª´ 20 splits...\")\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        split_name = f'split_{i:02d}' # Format 01, 02...\n",
    "        file_name = f'{mode}_full_lightcurves.csv'\n",
    "        full_path = os.path.join(base_path, split_name, file_name)\n",
    "        \n",
    "        print(f\"Processing {split_name}...\", end='\\r')\n",
    "        \n",
    "        feats = extract_features_from_split(full_path)\n",
    "        if feats is not None:\n",
    "            all_features.append(feats)\n",
    "            \n",
    "        # Gi·∫£i ph√≥ng b·ªô nh·ªõ RAM\n",
    "        del feats\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f\"\\nƒê√£ x·ª≠ l√Ω xong {mode}!\")\n",
    "    # G·ªôp t·∫•t c·∫£ c√°c split th√†nh 1 DataFrame l·ªõn\n",
    "    return pd.concat(all_features)\n",
    "\n",
    "# 3. Ch·∫°y x·ª≠ l√Ω d·ªØ li·ªáu (M·∫•t kho·∫£ng 2-5 ph√∫t)\n",
    "BASE_PATH = '/kaggle/input/mallorn-dataset'\n",
    "\n",
    "# 1. Load Feature t·ª´ Lightcurves (B∆∞·ªõc t·ªën th·ªùi gian nh·∫•t)\n",
    "train_lc_features = load_all_splits(BASE_PATH, mode='train')\n",
    "test_lc_features = load_all_splits(BASE_PATH, mode='test')\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc Train features:\", train_lc_features.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc Test features:\", test_lc_features.shape)\n",
    "\n",
    "# 2. Load file Log (Metadata)\n",
    "train_log = pd.read_csv(f'{BASE_PATH}/train_log.csv')\n",
    "test_log = pd.read_csv(f'{BASE_PATH}/test_log.csv')\n",
    "\n",
    "# 3. G·ªôp (Merge) Lightcurve Features v√†o Log Data\n",
    "# D√πng left join ƒë·ªÉ ƒë·∫£m b·∫£o gi·ªØ nguy√™n th·ª© t·ª± c·ªßa file Log\n",
    "full_train = train_log.merge(train_lc_features, on='object_id', how='left')\n",
    "full_test = test_log.merge(test_lc_features, on='object_id', how='left')\n",
    "\n",
    "# ƒêi·ªÅn 0 cho c√°c gi√° tr·ªã NaN sinh ra do merge (v√≠ d·ª• ng√¥i sao kh√¥ng c√≥ d·ªØ li·ªáu ·ªü filter 'u')\n",
    "full_train.fillna(0, inplace=True)\n",
    "full_test.fillna(0, inplace=True)\n",
    "\n",
    "display(full_train.head(3))\n",
    "\n",
    "# 4. Chu·∫©n b·ªã d·ªØ li·ªáu cho SVM (Scale & SMOTE)\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV # <--- V≈® KH√ç M·ªöI\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
    "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
    "\n",
    "X = full_train[feature_cols]\n",
    "y = full_train['target']\n",
    "X_test_sub = full_test[feature_cols]\n",
    "\n",
    "# Chia t·∫≠p (Stratify)\n",
    "X_train_org, X_val_org, y_train_org, y_val_org = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Hu·∫•n luy·ªán SVM & Tinh ch·ªânh (Training & Tuning)\n",
    "# Thay v√¨ ch·∫°y 1 l·∫ßn, ch√∫ng ta d√πng Cross-Validation ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ ·ªïn ƒë·ªãnh.\n",
    "\n",
    "# 2. Pipeline (Quay l·∫°i b·∫£n 0.3848)\n",
    "# L∆∞u √Ω: T√¥i b·ªè class_weight='balanced' v√¨ ƒë√£ c√≥ SMOTE. \n",
    "# D√πng c·∫£ 2 ƒë√¥i khi l√†m model b·ªã nhi·ªÖu (Double balancing).\n",
    "svm_pipeline = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),                 # Quay l·∫°i StandardScaler\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)), \n",
    "    ('select', SelectKBest(score_func=f_classif)),\n",
    "    ('svm', SVC(probability=True, kernel='rbf', random_state=42)) # B·ªè class_weight='balanced'\n",
    "])\n",
    "\n",
    "# 3. GridSearch\n",
    "param_grid = {\n",
    "    'select__k': [20, 30],\n",
    "    'svm__C': [10, 50, 100],      # TƒÉng C l√™n ƒë·ªÉ ph·∫°t l·ªói n·∫∑ng h∆°n\n",
    "    'svm__gamma': ['scale', 0.1],\n",
    "    'smote__sampling_strategy': [0.5, 0.75] # T·ª∑ l·ªá sinh v·ª´a ph·∫£i\n",
    "}\n",
    "\n",
    "print(\"ƒêang t√¨m Best SVM...\")\n",
    "grid = GridSearchCV(svm_pipeline, param_grid, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train_org, y_train_org)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_pipeline = grid.best_estimator_\n",
    "\n",
    "# --- B∆Ø·ªöC M·ªöI: HI·ªÜU CH·ªàNH X√ÅC SU·∫§T (CALIBRATION) ---\n",
    "# SVM thu·∫ßn th∆∞·ªùng t·ª± tin th√°i qu√° ho·∫∑c qu√° r·ª•t r√®. \n",
    "# CalibratedClassifierCV s·∫Ω \"n·∫Øn\" l·∫°i x√°c su·∫•t cho chu·∫©n.\n",
    "print(\"ƒêang hi·ªáu ch·ªânh x√°c su·∫•t (Calibration)...\")\n",
    "calibrated_svm = CalibratedClassifierCV(best_pipeline, method='isotonic', cv='prefit')\n",
    "calibrated_svm.fit(X_val_org, y_val_org) # Fit tr√™n t·∫≠p Val ƒë·ªÉ h·ªçc c√°ch s·ª≠a l·ªói\n",
    "\n",
    "# D·ª± ƒëo√°n th·ª≠ tr√™n Val\n",
    "y_pred_val_calibrated = calibrated_svm.predict(X_val_org)\n",
    "print(\"\\nValidation F1 (Calibrated):\", f1_score(y_val_org, y_pred_val_calibrated))\n",
    "\n",
    "# Tinh ch·ªânh ng∆∞·ª°ng (Threshold Tuning)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# L·∫•y x√°c su·∫•t t·ª´ model ƒë√£ hi·ªáu ch·ªânh\n",
    "y_val_prob = calibrated_svm.predict_proba(X_val_org)[:, 1]\n",
    "\n",
    "# T√¨m ng∆∞·ª°ng t·ªëi ∆∞u\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_org, y_val_prob)\n",
    "f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls)!=0)\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.4f}\")\n",
    "print(f\"Max F1 Val: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# 6. D·ª± ƒëo√°n v√† Submit\n",
    "# D·ª± ƒëo√°n Test\n",
    "y_test_prob = calibrated_svm.predict_proba(X_test_sub)[:, 1]\n",
    "final_predictions = (y_test_prob >= best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': full_test['object_id'],\n",
    "    'prediction': final_predictions\n",
    "})\n",
    "\n",
    "print(submission['prediction'].value_counts())\n",
    "submission.to_csv('submission_svm_calibrated.csv', index=False)\n",
    "print('ƒê√£ l∆∞u file: submission_svm_calibrated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 8\n",
    "# 0. Reset output\n",
    "!rm -rf /kaggle/working/*\n",
    "\n",
    "# 1. Import v√† c·∫•u h√¨nh¬∂\n",
    "# ===============================\n",
    "# üì¶ IMPORT TH∆Ø VI·ªÜN C∆† B·∫¢N\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# üìä VISUALIZATION\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# ü§ñ MACHINE LEARNING - SKLEARN\n",
    "# ===============================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ‚öñÔ∏è IMBALANCED LEARNING\n",
    "# ===============================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# 2. H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Core Engine)\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Reset h√†m feature t·ªët nh·∫•t\n",
    "from scipy.stats import skew\n",
    "\n",
    "def extract_features_from_split(csv_path):\n",
    "    if not os.path.exists(csv_path): return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1. T√çNH SNR (T√≠n hi·ªáu tr√™n nhi·ªÖu)\n",
    "    df['snr'] = df['Flux'] / (df['Flux_err'] + 1e-6)\n",
    "    \n",
    "    # 2. Th·ªëng k√™ c∆° b·∫£n\n",
    "    aggs = df.groupby(['object_id', 'Filter']).agg({\n",
    "        'Flux': ['max', 'min', 'mean', 'std', skew], # Skew r·∫•t quan tr·ªçng\n",
    "        'snr': ['max', 'mean']\n",
    "    }).unstack()\n",
    "    aggs.columns = [f'{col[0]}_{col[1]}_{col[2]}' for col in aggs.columns]\n",
    "    \n",
    "    # 3. T√≠nh M√†u & Bi√™n ƒë·ªô (V·∫≠t l√Ω)\n",
    "    if 'Flux_max_g' in aggs.columns and 'Flux_max_r' in aggs.columns:\n",
    "        aggs['color_g_r'] = aggs['Flux_max_g'] - aggs['Flux_max_r']\n",
    "        # Th√™m t·ª∑ l·ªá (Ratio) - SVM th√≠ch c√°i n√†y h∆°n hi·ªáu s·ªë\n",
    "        aggs['ratio_g_r'] = aggs['Flux_max_g'] / (aggs['Flux_max_r'] + 1)\n",
    "\n",
    "    filters = df['Filter'].unique()\n",
    "    for f in filters:\n",
    "        if f'Flux_max_{f}' in aggs.columns and f'Flux_min_{f}' in aggs.columns:\n",
    "            aggs[f'amp_{f}'] = aggs[f'Flux_max_{f}'] - aggs[f'Flux_min_{f}']\n",
    "            \n",
    "    # 4. S·ªë l∆∞·ª£ng quan s√°t\n",
    "    counts = df.groupby('object_id').size().to_frame('n_obs')\n",
    "    \n",
    "    # Merge l·∫°i\n",
    "    features = aggs.merge(counts, left_index=True, right_index=True)\n",
    "    return features\n",
    "\n",
    "# --- CH·∫†Y L·∫†I B∆Ø·ªöC LOAD D·ªÆ LI·ªÜU ---\n",
    "# (B·∫°n nh·ªõ ch·∫°y l·∫°i h√†m load_all_splits v√† t·∫°o full_train, full_test nh√©)\n",
    "# ...\n",
    "\n",
    "# H√†m h·ªó tr·ª£ load to√†n b·ªô 20 splits\n",
    "def load_all_splits(base_path, mode='train'):\n",
    "    all_features = []\n",
    "    print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu {mode} t·ª´ 20 splits...\")\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        split_name = f'split_{i:02d}' # Format 01, 02...\n",
    "        file_name = f'{mode}_full_lightcurves.csv'\n",
    "        full_path = os.path.join(base_path, split_name, file_name)\n",
    "        \n",
    "        print(f\"Processing {split_name}...\", end='\\r')\n",
    "        \n",
    "        feats = extract_features_from_split(full_path)\n",
    "        if feats is not None:\n",
    "            all_features.append(feats)\n",
    "            \n",
    "        # Gi·∫£i ph√≥ng b·ªô nh·ªõ RAM\n",
    "        del feats\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f\"\\nƒê√£ x·ª≠ l√Ω xong {mode}!\")\n",
    "    # G·ªôp t·∫•t c·∫£ c√°c split th√†nh 1 DataFrame l·ªõn\n",
    "    return pd.concat(all_features)\n",
    "\n",
    "# 3. Ch·∫°y x·ª≠ l√Ω d·ªØ li·ªáu (M·∫•t kho·∫£ng 2-5 ph√∫t)\n",
    "BASE_PATH = '/kaggle/input/mallorn-dataset'\n",
    "\n",
    "# 1. Load Feature t·ª´ Lightcurves (B∆∞·ªõc t·ªën th·ªùi gian nh·∫•t)\n",
    "train_lc_features = load_all_splits(BASE_PATH, mode='train')\n",
    "test_lc_features = load_all_splits(BASE_PATH, mode='test')\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc Train features:\", train_lc_features.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc Test features:\", test_lc_features.shape)\n",
    "\n",
    "# 2. Load file Log (Metadata)\n",
    "train_log = pd.read_csv(f'{BASE_PATH}/train_log.csv')\n",
    "test_log = pd.read_csv(f'{BASE_PATH}/test_log.csv')\n",
    "\n",
    "# 3. G·ªôp (Merge) Lightcurve Features v√†o Log Data\n",
    "# D√πng left join ƒë·ªÉ ƒë·∫£m b·∫£o gi·ªØ nguy√™n th·ª© t·ª± c·ªßa file Log\n",
    "full_train = train_log.merge(train_lc_features, on='object_id', how='left')\n",
    "full_test = test_log.merge(test_lc_features, on='object_id', how='left')\n",
    "\n",
    "# ƒêi·ªÅn 0 cho c√°c gi√° tr·ªã NaN sinh ra do merge (v√≠ d·ª• ng√¥i sao kh√¥ng c√≥ d·ªØ li·ªáu ·ªü filter 'u')\n",
    "full_train.fillna(0, inplace=True)\n",
    "full_test.fillna(0, inplace=True)\n",
    "\n",
    "display(full_train.head(3))\n",
    "\n",
    "# 4. Chu·∫©n b·ªã d·ªØ li·ªáu cho SVM (Scale & SMOTE)\n",
    "# --- B∆Ø·ªöC 4: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (S·ª¨A L·∫†I) ---\n",
    "# Ch·ªçn Feature\n",
    "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
    "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
    "X = full_train[feature_cols]\n",
    "y = full_train['target']\n",
    "X_test_sub = full_test[feature_cols]\n",
    "\n",
    "# Chia t·∫≠p Train/Val G·ªêC (Ch∆∞a scale, ch∆∞a SMOTE)\n",
    "# Stratify ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá TDE ·ªü 2 t·∫≠p nh∆∞ nhau\n",
    "X_train_org, X_val_org, y_train_org, y_val_org = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng TDE trong t·∫≠p Val th·ª±c t·∫ø: {sum(y_val_org==1)}\")\n",
    "\n",
    "# 5. Hu·∫•n luy·ªán SVM & Tinh ch·ªânh (Training & Tuning)\n",
    "# changes for v8: Bagging + RobustScaler + RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import RobustScaler  # <--- THAY ƒê·ªîI 1\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, precision_recall_curve\n",
    "\n",
    "# 1. CHU·∫®N B·ªä D·ªÆ LI·ªÜU\n",
    "# L·∫•y l·∫°i d·ªØ li·ªáu g·ªëc t·ª´ full_train (ƒë√£ ch·∫°y qua h√†m extract_features chu·∫©n ·ªü b∆∞·ªõc tr∆∞·ªõc)\n",
    "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
    "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
    "\n",
    "X = full_train[feature_cols]\n",
    "y = full_train['target']\n",
    "X_test_sub = full_test[feature_cols]\n",
    "\n",
    "# Chia t·∫≠p Train/Val (Stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá TDE)\n",
    "X_train_org, X_val_org, y_train_org, y_val_org = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. X√ÇY D·ª∞NG PIPELINE \"BAGGING SVM\"\n",
    "# SVM c∆° s·ªü (Base Estimator)\n",
    "svc_base = SVC(\n",
    "    kernel='rbf', \n",
    "    probability=True,       # B·∫Øt bu·ªôc True ƒë·ªÉ Bagging t√≠nh ƒë∆∞·ª£c x√°c su·∫•t trung b√¨nh\n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_pipeline = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),    # D√πng Median an to√†n h∆°n Mean\n",
    "    ('scaler', RobustScaler()),                       # D√πng RobustScaler ch·ªëng nhi·ªÖu\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=7)), # TƒÉng neighbor l√™n 7 cho m∆∞·ª£t\n",
    "    ('select', SelectKBest(score_func=f_classif)),    \n",
    "    ('bagging', BaggingClassifier(                    # <--- THAY ƒê·ªîI 2: B·ªçc trong Bagging\n",
    "        estimator=svc_base,\n",
    "        n_estimators=10,        # T·∫°o 10 model con (Ensemble)\n",
    "        max_samples=0.8,        # M·ªói model h·ªçc 80% d·ªØ li·ªáu\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1               # Ch·∫°y song song\n",
    "    )) \n",
    "])\n",
    "\n",
    "# 3. T√åM THAM S·ªê (GridSearch/RandomizedSearch)\n",
    "# L∆∞u √Ω c√∫ ph√°p: t√™n_b∆∞·ªõc__t√™n_tham_s·ªë\n",
    "# V·ªõi Bagging: bagging__estimator__tham_s·ªë_c·ªßa_svm\n",
    "param_grid = {\n",
    "    'select__k': [20, 30, 'all'],             # S·ªë l∆∞·ª£ng feature gi·ªØ l·∫°i\n",
    "    'bagging__estimator__C': [10, 50, 100],   # C c·ªßa SVM con (tƒÉng l√™n v√¨ RobustScaler thu nh·ªè data)\n",
    "    'bagging__estimator__gamma': ['scale', 0.1],\n",
    "    'smote__sampling_strategy': [0.5, 0.8]    # T·ª∑ l·ªá sinh m·∫´u\n",
    "}\n",
    "\n",
    "print(\"ƒêang ch·∫°y RandomizedSearchCV cho Bagging SVM...\")\n",
    "search = RandomizedSearchCV(\n",
    "    svm_pipeline, \n",
    "    param_grid, \n",
    "    n_iter=10,    # Th·ª≠ 10 t·ªï h·ª£p t·ªët nh·∫•t\n",
    "    cv=3, \n",
    "    scoring='f1', \n",
    "    verbose=2, \n",
    "    random_state=42,\n",
    "    n_jobs=1      # ƒê·ªÉ 1 v√¨ b√™n trong Bagging ƒë√£ d√πng ƒëa nh√¢n r·ªìi\n",
    ")\n",
    "\n",
    "search.fit(X_train_org, y_train_org)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "print(\"\\nBest params:\", search.best_params_)\n",
    "\n",
    "# Tinh ch·ªânh ng∆∞·ª°ng (Threshold Tuning)\n",
    "# 4. T√åM NG∆Ø·ª†NG T·ªêI ∆ØU (Threshold Tuning)\n",
    "# L·∫•y x√°c su·∫•t d·ª± ƒëo√°n tr√™n t·∫≠p Val\n",
    "y_val_prob = best_model.predict_proba(X_val_org)[:, 1]\n",
    "\n",
    "# T√≠nh ƒë∆∞·ªùng cong P-R\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_org, y_val_prob)\n",
    "f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls)!=0)\n",
    "\n",
    "# Ch·ªçn ng∆∞·ª°ng c√≥ F1 cao nh·∫•t\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.4f}\")\n",
    "print(f\"Validation F1 (Max): {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# 6. D·ª± ƒëo√°n v√† Submit\n",
    "# 5. SUBMIT\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p Test\n",
    "y_test_prob = best_model.predict_proba(X_test_sub)[:, 1]\n",
    "final_predictions = (y_test_prob >= best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': full_test['object_id'],\n",
    "    'prediction': final_predictions\n",
    "})\n",
    "\n",
    "print(\"\\nTh·ªëng k√™ d·ª± ƒëo√°n:\")\n",
    "print(submission['prediction'].value_counts())\n",
    "\n",
    "submission.to_csv('submission_svm_bagging_robust.csv', index=False)\n",
    "print(\"ƒê√£ l∆∞u file: submission_svm_bagging_robust.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b055d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 6\n",
    "# 0. Reset output\n",
    "!rm -rf /kaggle/working/*\n",
    "\n",
    "# 1. Import v√† c·∫•u h√¨nh\n",
    "# ===============================\n",
    "# üì¶ IMPORT TH∆Ø VI·ªÜN C∆† B·∫¢N\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# üìä VISUALIZATION\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# ü§ñ MACHINE LEARNING - SKLEARN\n",
    "# ===============================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ‚öñÔ∏è IMBALANCED LEARNING\n",
    "# ===============================\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# 2. H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Core Engine)\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Reset h√†m feature t·ªët nh·∫•t\n",
    "from scipy.stats import skew\n",
    "\n",
    "def extract_features_from_split(csv_path):\n",
    "    if not os.path.exists(csv_path): return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1. T√çNH SNR (T√≠n hi·ªáu tr√™n nhi·ªÖu)\n",
    "    df['snr'] = df['Flux'] / (df['Flux_err'] + 1e-6)\n",
    "    \n",
    "    # 2. Th·ªëng k√™ c∆° b·∫£n\n",
    "    aggs = df.groupby(['object_id', 'Filter']).agg({\n",
    "        'Flux': ['max', 'min', 'mean', 'std', skew], # Skew r·∫•t quan tr·ªçng\n",
    "        'snr': ['max', 'mean']\n",
    "    }).unstack()\n",
    "    aggs.columns = [f'{col[0]}_{col[1]}_{col[2]}' for col in aggs.columns]\n",
    "    \n",
    "    # 3. T√≠nh M√†u & Bi√™n ƒë·ªô (V·∫≠t l√Ω)\n",
    "    if 'Flux_max_g' in aggs.columns and 'Flux_max_r' in aggs.columns:\n",
    "        aggs['color_g_r'] = aggs['Flux_max_g'] - aggs['Flux_max_r']\n",
    "        # Th√™m t·ª∑ l·ªá (Ratio) - SVM th√≠ch c√°i n√†y h∆°n hi·ªáu s·ªë\n",
    "        aggs['ratio_g_r'] = aggs['Flux_max_g'] / (aggs['Flux_max_r'] + 1)\n",
    "\n",
    "    filters = df['Filter'].unique()\n",
    "    for f in filters:\n",
    "        if f'Flux_max_{f}' in aggs.columns and f'Flux_min_{f}' in aggs.columns:\n",
    "            aggs[f'amp_{f}'] = aggs[f'Flux_max_{f}'] - aggs[f'Flux_min_{f}']\n",
    "            \n",
    "    # 4. S·ªë l∆∞·ª£ng quan s√°t\n",
    "    counts = df.groupby('object_id').size().to_frame('n_obs')\n",
    "    \n",
    "    # Merge l·∫°i\n",
    "    features = aggs.merge(counts, left_index=True, right_index=True)\n",
    "    return features\n",
    "\n",
    "# H√†m h·ªó tr·ª£ load to√†n b·ªô 20 splits\n",
    "def load_all_splits(base_path, mode='train'):\n",
    "    all_features = []\n",
    "    print(f\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu {mode} t·ª´ 20 splits...\")\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "        split_name = f'split_{i:02d}' # Format 01, 02...\n",
    "        file_name = f'{mode}_full_lightcurves.csv'\n",
    "        full_path = os.path.join(base_path, split_name, file_name)\n",
    "        \n",
    "        print(f\"Processing {split_name}...\", end='\\r')\n",
    "        \n",
    "        feats = extract_features_from_split(full_path)\n",
    "        if feats is not None:\n",
    "            all_features.append(feats)\n",
    "            \n",
    "        # Gi·∫£i ph√≥ng b·ªô nh·ªõ RAM\n",
    "        del feats\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f\"\\nƒê√£ x·ª≠ l√Ω xong {mode}!\")\n",
    "    # G·ªôp t·∫•t c·∫£ c√°c split th√†nh 1 DataFrame l·ªõn\n",
    "    return pd.concat(all_features)\n",
    "\n",
    "# 3. Ch·∫°y x·ª≠ l√Ω d·ªØ li·ªáu (M·∫•t kho·∫£ng 2-5 ph√∫t)\n",
    "BASE_PATH = '/kaggle/input/mallorn-dataset'\n",
    "\n",
    "# 1. Load Feature t·ª´ Lightcurves (B∆∞·ªõc t·ªën th·ªùi gian nh·∫•t)\n",
    "train_lc_features = load_all_splits(BASE_PATH, mode='train')\n",
    "test_lc_features = load_all_splits(BASE_PATH, mode='test')\n",
    "\n",
    "print(\"K√≠ch th∆∞·ªõc Train features:\", train_lc_features.shape)\n",
    "print(\"K√≠ch th∆∞·ªõc Test features:\", test_lc_features.shape)\n",
    "\n",
    "# 2. Load file Log (Metadata)\n",
    "train_log = pd.read_csv(f'{BASE_PATH}/train_log.csv')\n",
    "test_log = pd.read_csv(f'{BASE_PATH}/test_log.csv')\n",
    "\n",
    "# 3. G·ªôp (Merge) Lightcurve Features v√†o Log Data\n",
    "# D√πng left join ƒë·ªÉ ƒë·∫£m b·∫£o gi·ªØ nguy√™n th·ª© t·ª± c·ªßa file Log\n",
    "full_train = train_log.merge(train_lc_features, on='object_id', how='left')\n",
    "full_test = test_log.merge(test_lc_features, on='object_id', how='left')\n",
    "\n",
    "# ƒêi·ªÅn 0 cho c√°c gi√° tr·ªã NaN sinh ra do merge (v√≠ d·ª• ng√¥i sao kh√¥ng c√≥ d·ªØ li·ªáu ·ªü filter 'u')\n",
    "full_train.fillna(0, inplace=True)\n",
    "full_test.fillna(0, inplace=True)\n",
    "\n",
    "display(full_train.head(3))\n",
    "\n",
    "# 4. Chu·∫©n b·ªã d·ªØ li·ªáu cho SVM (Scale & SMOTE)\n",
    "# --- B∆Ø·ªöC 4: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (S·ª¨A L·∫†I) ---\n",
    "# Ch·ªçn Feature\n",
    "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
    "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
    "X = full_train[feature_cols]\n",
    "y = full_train['target']\n",
    "X_test_sub = full_test[feature_cols]\n",
    "\n",
    "# Chia t·∫≠p Train/Val G·ªêC (Ch∆∞a scale, ch∆∞a SMOTE)\n",
    "# Stratify ƒë·ªÉ ƒë·∫£m b·∫£o t·ª∑ l·ªá TDE ·ªü 2 t·∫≠p nh∆∞ nhau\n",
    "X_train_org, X_val_org, y_train_org, y_val_org = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng TDE trong t·∫≠p Val th·ª±c t·∫ø: {sum(y_val_org==1)}\")\n",
    "\n",
    "# 5. Hu·∫•n luy·ªán SVM & Tinh ch·ªânh (Training & Tuning)\n",
    "svm_pipeline = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),      # 1. ƒêi·ªÅn gi√° tr·ªã thi·∫øu\n",
    "    ('scaler', StandardScaler()),                     # 2. Chu·∫©n h√≥a\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)), # 3. Sinh d·ªØ li·ªáu (ch·ªâ tr√™n train)\n",
    "    ('select', SelectKBest(score_func=f_classif)),    # 4. Ch·ªçn ƒë·∫∑c tr∆∞ng t·ªët nh·∫•t\n",
    "    ('svm', SVC(probability=True, kernel='rbf', class_weight='balanced', random_state=42)) # 5. SVM\n",
    "])\n",
    "\n",
    "# Thi·∫øt l·∫≠p tham s·ªë GridSearch\n",
    "param_grid = {\n",
    "    'select__k': [20, 30, 'all'],           # Gi·ªØ l·∫°i 20, 30 ho·∫∑c t·∫•t c·∫£ feature\n",
    "    'svm__C': [1, 10, 100],                 # Tham s·ªë C c·ªßa SVM\n",
    "    'svm__gamma': ['scale', 0.1],           # Tham s·ªë Gamma\n",
    "    'smote__sampling_strategy': [0.5, 1.0]  # T·ª∑ l·ªá sinh d·ªØ li·ªáu\n",
    "}\n",
    "\n",
    "print(\"ƒêang ch·∫°y GridSearch cho SVM (Quy tr√¨nh chu·∫©n)...\")\n",
    "# cv=3: Chia 3 fold ki·ªÉm tra ch√©o\n",
    "grid = GridSearchCV(svm_pipeline, param_grid, cv=3, scoring='f1', verbose=2, n_jobs=-1)\n",
    "\n",
    "# L∆∞u √Ω: D√πng X_train_org v√† y_train_org (D·ªØ li·ªáu G·ªêC ch∆∞a qua x·ª≠ l√Ω)\n",
    "grid.fit(X_train_org, y_train_org)\n",
    "\n",
    "print(\"\\nBest params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# --- ƒê√ÅNH GI√Å TRUNG TH·ª∞C ---\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p Val g·ªëc (Pipeline s·∫Ω t·ª± scale, b·∫°n ch·ªâ c·∫ßn ƒë∆∞a d·ªØ li·ªáu th√¥ v√†o)\n",
    "y_pred_val = best_model.predict(X_val_org)\n",
    "print(\"\\nValidation F1 Score (Real):\", f1_score(y_val_org, y_pred_val))\n",
    "print(classification_report(y_val_org, y_pred_val))\n",
    "\n",
    "# Tinh ch·ªânh l·∫ßn 2 (thu nh·ªè grid)\n",
    "svm_pipeline = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)), \n",
    "    ('select', SelectKBest(score_func=f_classif, k=20)), # CH·ªà GI·ªÆ L·∫†I 20 FEATURE T·ªêT NH·∫§T\n",
    "    ('svm', SVC(probability=True, kernel='rbf', class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': [15, 25, 'all'],     # Th·ª≠ ch·ªçn 15, 25 ho·∫∑c l·∫•y h·∫øt feature\n",
    "    'svm__C': [1, 10, 50],            # C c√†ng l·ªõn c√†ng ph·∫°t l·ªói m·∫°nh (d·ªÖ overfit)\n",
    "    'svm__gamma': ['scale', 0.01, 0.1],\n",
    "    'smote__sampling_strategy': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"ƒêang ch·∫°y GridSearch cho SVM (Quy tr√¨nh chu·∫©n)...\")\n",
    "grid = GridSearchCV(svm_pipeline, param_grid, cv=3, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train_org, y_train_org)\n",
    "\n",
    "print(\"\\nBest params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# --- ƒê√ÅNH GI√Å TRUNG TH·ª∞C ---\n",
    "y_pred_val = best_model.predict(X_val_org)\n",
    "print(\"\\nValidation F1 Score (Real):\", f1_score(y_val_org, y_pred_val))\n",
    "print(classification_report(y_val_org, y_pred_val))\n",
    "\n",
    "# Tinh ch·ªânh ng∆∞·ª°ng (Threshold Tuning)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# --- 1. T√åM NG∆Ø·ª†NG T·ªêI ∆ØU (Threshold Tuning) ---\n",
    "# D·ª± ƒëo√°n x√°c su·∫•t tr√™n t·∫≠p Validation\n",
    "y_val_prob = best_model.predict_proba(X_val_org)[:, 1]\n",
    "\n",
    "# T√≠nh Precision-Recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_org, y_val_prob)\n",
    "\n",
    "# T√≠nh F1 score cho t·ª´ng ng∆∞·ª°ng (X·ª≠ l√Ω chia cho 0 ƒë·ªÉ tr√°nh l·ªói)\n",
    "f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls)!=0)\n",
    "\n",
    "# L·∫•y ng∆∞·ª°ng c√≥ F1 cao nh·∫•t\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Ng∆∞·ª°ng t·ªëi ∆∞u (Best Threshold): {best_threshold:.4f}\")\n",
    "print(f\"F1 Score k·ª≥ v·ªçng t·∫°i ng∆∞·ª°ng n√†y: {best_f1:.4f}\")\n",
    "\n",
    "# 6. D·ª± ƒëo√°n v√† Submit\n",
    "# --- 2. D·ª∞ ƒêO√ÅN V√Ä SUBMIT ---\n",
    "\n",
    "# D·ª± ƒëo√°n x√°c su·∫•t tr√™n t·∫≠p Test th·∫≠t\n",
    "# L∆ØU √ù QUAN TR·ªåNG: D√πng X_test_sub (d·ªØ li·ªáu th√¥), KH√îNG d√πng X_test_scaled\n",
    "y_test_prob = best_model.predict_proba(X_test_sub)[:, 1]\n",
    "\n",
    "# √Åp d·ª•ng ng∆∞·ª°ng t·ªëi ∆∞u\n",
    "final_predictions = (y_test_prob >= best_threshold).astype(int)\n",
    "\n",
    "# T·∫°o file submission\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': full_test['object_id'],\n",
    "    'prediction': final_predictions\n",
    "})\n",
    "\n",
    "# Ki·ªÉm tra ph√¢n ph·ªëi d·ª± ƒëo√°n\n",
    "print(\"\\nTh·ªëng k√™ d·ª± ƒëo√°n (Test Set):\")\n",
    "print(submission['prediction'].value_counts())\n",
    "\n",
    "submission.to_csv('submission_svm_final.csv', index=False)\n",
    "print(\"ƒê√£ l∆∞u file: submission_svm_final.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
