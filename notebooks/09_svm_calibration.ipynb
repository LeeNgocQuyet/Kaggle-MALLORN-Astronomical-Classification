{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 07: SVM with Probability Calibration\n",
                "\n",
                "## Overview\n",
                "This notebook focuses on improving the reliability of the model's probability outputs.\n",
                "- **Feature Extraction**: SNR + Color + Amplitude + Basic Stats.\n",
                "- **Pipeline**: Imputer -> StandardScaler -> SMOTE -> SelectKBest -> SVC.\n",
                "- **New Step**: `CalibratedClassifierCV` (Isotonic) to refine probabilities.\n",
                "- **Evaluation**: Threshold Tuning on Calibrated Probabilities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Import & Config\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import os\n",
                "import gc\n",
                "import warnings\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.feature_selection import SelectKBest, f_classif\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.calibration import CalibratedClassifierCV\n",
                "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
                "\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Feature Extraction\n",
                "from scipy.stats import skew\n",
                "\n",
                "def extract_features_from_split(csv_path):\n",
                "    if not os.path.exists(csv_path): return None\n",
                "    df = pd.read_csv(csv_path)\n",
                "    \n",
                "    # 1. TÍNH SNR\n",
                "    df['snr'] = df['Flux'] / (df['Flux_err'] + 1e-6)\n",
                "    \n",
                "    # 2. Thống kê\n",
                "    aggs = df.groupby(['object_id', 'Filter']).agg({\n",
                "        'Flux': ['max', 'min', 'mean', 'std', skew],\n",
                "        'snr': ['max', 'mean']\n",
                "    }).unstack()\n",
                "    aggs.columns = [f'{col[0]}_{col[1]}_{col[2]}' for col in aggs.columns]\n",
                "    \n",
                "    # 3. Tính Màu (Quan trọng)\n",
                "    if 'Flux_max_g' in aggs.columns and 'Flux_max_r' in aggs.columns:\n",
                "        aggs['color_g_r'] = aggs['Flux_max_g'] - aggs['Flux_max_r']\n",
                "    \n",
                "    # 4. Biên độ\n",
                "    filters = df['Filter'].unique()\n",
                "    for f in filters:\n",
                "        if f'Flux_max_{f}' in aggs.columns and f'Flux_min_{f}' in aggs.columns:\n",
                "            aggs[f'amp_{f}'] = aggs[f'Flux_max_{f}'] - aggs[f'Flux_min_{f}']\n",
                "            \n",
                "    # 5. Số lượng quan sát\n",
                "    counts = df.groupby('object_id').size().to_frame('n_obs')\n",
                "    \n",
                "    features = aggs.merge(counts, left_index=True, right_index=True)\n",
                "    return features\n",
                "\n",
                "def load_all_splits(base_path, mode='train'):\n",
                "    all_features = []\n",
                "    print(f\"Bắt đầu xử lý dữ liệu {mode} từ 20 splits...\")\n",
                "    \n",
                "    for i in range(1, 21):\n",
                "        split_name = f'split_{i:02d}' # Format 01, 02...\n",
                "        file_name = f'{mode}_full_lightcurves.csv'\n",
                "        full_path = os.path.join(base_path, split_name, file_name)\n",
                "        \n",
                "        print(f\"Processing {split_name}...\", end='\\r')\n",
                "        \n",
                "        feats = extract_features_from_split(full_path)\n",
                "        if feats is not None:\n",
                "            all_features.append(feats)\n",
                "            \n",
                "        # Giải phóng bộ nhớ RAM\n",
                "        del feats\n",
                "        gc.collect()\n",
                "        \n",
                "    print(f\"\\nĐã xử lý xong {mode}!\")\n",
                "    # Gộp tất cả các split thành 1 DataFrame lớn\n",
                "    return pd.concat(all_features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Data\n",
                "BASE_PATH = 'data/raw'\n",
                "\n",
                "print(\"Loading Train features...\")\n",
                "train_lc_features = load_all_splits(BASE_PATH, mode='train')\n",
                "test_lc_features = load_all_splits(BASE_PATH, mode='test')\n",
                "\n",
                "train_log = pd.read_csv(os.path.join(BASE_PATH, 'train_log.csv'))\n",
                "test_log = pd.read_csv(os.path.join(BASE_PATH, 'test_log.csv'))\n",
                "\n",
                "full_train = train_log.merge(train_lc_features, on='object_id', how='left')\n",
                "full_test = test_log.merge(test_lc_features, on='object_id', how='left')\n",
                "\n",
                "full_train.fillna(0, inplace=True)\n",
                "full_test.fillna(0, inplace=True)\n",
                "\n",
                "display(full_train.head(3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Prepare Data\n",
                "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
                "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
                "\n",
                "X = full_train[feature_cols]\n",
                "y = full_train['target']\n",
                "X_test_sub = full_test[feature_cols]\n",
                "\n",
                "# Split (Stratify)\n",
                "X_train_org, X_val_org, y_train_org, y_val_org = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"TDE in Val: {sum(y_val_org==1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Pipeline & Calibration\n",
                "# Removing class_weight='balanced' as requested\n",
                "svm_pipeline = ImbPipeline([\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler()),                 \n",
                "    ('smote', SMOTE(random_state=42, k_neighbors=5)), \n",
                "    ('select', SelectKBest(score_func=f_classif)),\n",
                "    ('svm', SVC(probability=True, kernel='rbf', random_state=42)) \n",
                "])\n",
                "\n",
                "# GridSearch\n",
                "param_grid = {\n",
                "    'select__k': [20, 30],\n",
                "    'svm__C': [10, 50, 100],\n",
                "    'svm__gamma': ['scale', 0.1],\n",
                "    'smote__sampling_strategy': [0.5, 0.75]\n",
                "}\n",
                "\n",
                "print(\"Finding Best SVM...\")\n",
                "grid = GridSearchCV(svm_pipeline, param_grid, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
                "grid.fit(X_train_org, y_train_org)\n",
                "\n",
                "print(\"Best params:\", grid.best_params_)\n",
                "best_pipeline = grid.best_estimator_\n",
                "\n",
                "# --- CALIBRATION ---\n",
                "print(\"Calibrating probabilities...\")\n",
                "# cv='prefit' means we calibrate on the already fitted model using the validation set\n",
                "calibrated_svm = CalibratedClassifierCV(best_pipeline, method='isotonic', cv='prefit')\n",
                "calibrated_svm.fit(X_val_org, y_val_org)\n",
                "\n",
                "y_pred_val_calibrated = calibrated_svm.predict(X_val_org)\n",
                "print(\"\\nValidation F1 (Calibrated):\", f1_score(y_val_org, y_pred_val_calibrated))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Threshold Tuning\n",
                "y_val_prob = calibrated_svm.predict_proba(X_val_org)[:, 1]\n",
                "\n",
                "precisions, recalls, thresholds = precision_recall_curve(y_val_org, y_val_prob)\n",
                "f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls)!=0)\n",
                "\n",
                "best_idx = np.argmax(f1_scores)\n",
                "best_threshold = thresholds[best_idx]\n",
                "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
                "print(f\"Max F1 Val: {f1_scores[best_idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Submission\n",
                "y_test_prob = calibrated_svm.predict_proba(X_test_sub)[:, 1]\n",
                "final_predictions = (y_test_prob >= best_threshold).astype(int)\n",
                "\n",
                "submission = pd.DataFrame({\n",
                "    'object_id': full_test['object_id'],\n",
                "    'prediction': final_predictions\n",
                "})\n",
                "\n",
                "print(submission['prediction'].value_counts())\n",
                "submission.to_csv('submission_svm_calibrated.csv', index=False)\n",
                "print(\"Done.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}