{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 02: XGBoost Classification Pipeline (GPU Enabed)\n",
                "\n",
                "## Overview\n",
                "This notebook implements the XGBoost pipeline exactly as requested.\n",
                "- **Core**: Feature Extraction (User Engine).\n",
                "- **Preprocessing**: StandardScaler + SMOTE.\n",
                "- **Model**: XGBClassifier (GPU/Hist) with RandomizedSearchCV.\n",
                "- **Refinement**: Threshold Tuning.\n",
                "\n",
                "> **Note**: This notebook requires a GPU environment (e.g. Kaggle T4, Colab). If running on CPU, change `device='cuda'` to `device='cpu'`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Import & Config\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import gc\n",
                "import time\n",
                "import warnings\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
                "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
                "from sklearn.impute import SimpleImputer\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "# Custom module (Shared with SVM)\n",
                "from src.data_processing import load_all_splits\n",
                "\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Feature Extraction\n",
                "BASE_PATH = 'data/raw'\n",
                "\n",
                "print(\"Loading Train features...\")\n",
                "train_lc_features = load_all_splits(BASE_PATH, mode='train')\n",
                "\n",
                "print(\"Loading Test features...\")\n",
                "test_lc_features = load_all_splits(BASE_PATH, mode='test')\n",
                "\n",
                "print(\"Shape Train:\", train_lc_features.shape)\n",
                "print(\"Shape Test:\", test_lc_features.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Merge Metadata\n",
                "train_log = pd.read_csv(os.path.join(BASE_PATH, 'train_log.csv'))\n",
                "test_log = pd.read_csv(os.path.join(BASE_PATH, 'test_log.csv'))\n",
                "\n",
                "full_train = train_log.merge(train_lc_features, on='object_id', how='left')\n",
                "full_test = test_log.merge(test_lc_features, on='object_id', how='left')\n",
                "\n",
                "full_train.fillna(0, inplace=True)\n",
                "full_test.fillna(0, inplace=True)\n",
                "\n",
                "display(full_train.head(3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Preprocessing (Scale & SMOTE)\n",
                "drop_cols = ['object_id', 'SpecType', 'English Translation', 'split', 'target', 'Z_err']\n",
                "feature_cols = [c for c in full_train.columns if c not in drop_cols]\n",
                "\n",
                "print(f\"Using {len(feature_cols)} features.\")\n",
                "\n",
                "X = full_train[feature_cols]\n",
                "y = full_train['target']\n",
                "X_test_final = full_test[feature_cols]\n",
                "\n",
                "# Scaling (StandardScaler as requested)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "X_test_scaled = scaler.transform(X_test_final)\n",
                "\n",
                "# SMOTE\n",
                "print(f\"Original TDE: {sum(y==1)}\")\n",
                "smote = SMOTE(random_state=42)\n",
                "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
                "print(f\"After SMOTE TDE: {sum(y_resampled==1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. XGBoost Training\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
                "\n",
                "# Config GPU XGBoost\n",
                "xgb_gpu = XGBClassifier(\n",
                "    tree_method='hist',\n",
                "    device='cuda',        # Run on GPU\n",
                "    eval_metric='logloss',\n",
                "    random_state=42,\n",
                "    use_label_encoder=False,\n",
                "    early_stopping_rounds=None\n",
                ")\n",
                "\n",
                "# Param Grid\n",
                "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
                "param_grid = {\n",
                "    'n_estimators': [500, 800, 1200],\n",
                "    'learning_rate': [0.01, 0.05, 0.1],\n",
                "    'max_depth': [6, 8, 10],\n",
                "    'subsample': [0.8, 0.9],\n",
                "    'colsample_bytree': [0.8, 0.9],\n",
                "    'scale_pos_weight': [ratio, ratio * 1.2]\n",
                "}\n",
                "\n",
                "# RandomizedSearch\n",
                "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
                "print(\"Running RandomizedSearchCV on GPU...\")\n",
                "start_time = time.time()\n",
                "\n",
                "search = RandomizedSearchCV(\n",
                "    estimator=xgb_gpu,\n",
                "    param_distributions=param_grid,\n",
                "    n_iter=20,\n",
                "    scoring='f1',\n",
                "    cv=kf,\n",
                "    verbose=1,\n",
                "    random_state=42,\n",
                "    n_jobs=1 # Required for GPU safety\n",
                ")\n",
                "\n",
                "search.fit(X_train, y_train)\n",
                "print(f\"Done in {(time.time() - start_time)/60:.2f} min.\")\n",
                "print(\"Best Params:\", search.best_params_)\n",
                "\n",
                "best_model = search.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Evaluation\n",
                "y_pred_val = best_model.predict(X_val)\n",
                "print(\"Validation F1 (Default 0.5):\", f1_score(y_val, y_pred_val))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Threshold Tuning\n",
                "y_val_prob = best_model.predict_proba(X_val)[:, 1]\n",
                "\n",
                "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_prob)\n",
                "f1_scores = np.divide(2 * precisions * recalls, precisions + recalls, out=np.zeros_like(precisions), where=(precisions + recalls)!=0)\n",
                "\n",
                "best_idx = np.argmax(f1_scores)\n",
                "best_threshold = thresholds[best_idx]\n",
                "best_f1 = f1_scores[best_idx]\n",
                "\n",
                "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
                "print(f\"Best F1: {best_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Submission\n",
                "y_test_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
                "final_predictions = (y_test_prob >= best_threshold).astype(int)\n",
                "\n",
                "submission = pd.DataFrame({\n",
                "    'object_id': full_test['object_id'],\n",
                "    'prediction': final_predictions\n",
                "})\n",
                "\n",
                "print(submission['prediction'].value_counts())\n",
                "submission.to_csv('submission_xgboost_gpu.csv', index=False)\n",
                "print(\"Done.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}